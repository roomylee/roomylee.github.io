---
title: "[WIP] ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (ICLR 2020)"
layout: post
date: 2020-04-13
headerImage: false
tag:
- electra
- pre-trained-language-model
- pre-training
- masked-language-modeling
- replaced-token-detection
- generator-discriminator
category: blog
author: roomylee
---

- Paper Link: <https://openreview.net/pdf?id=r1xMH1BtvB>
- Author
  - Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning
  - Stanford University & Google Brain
- Published at
  - ICLR 2020

---

## Abstract

- Masked language modeling은 대표적으로 BERT가 input을 $[MASK]$로 치환하고 이를 원본 token으로 재복원하는 방식의 pre-training method임
- Downstream task으로 transferring 했을 때 좋은 성능을 보이긴 하지만 많은 양의 계산 비용이 듬
- 이런 문제를 해결하기 위해서, **replaced token detection** 이라는 sample-efficient pre-training task를 제안함
- 이 기법은 input을 masking하는 것 대신에 small generator network로부터 sampling 된 그럴싸한 token으로 치환함
- 그 다음, 모델이 masked token의 원본을 예측하면서 학습하는 것 대신에 discriminative model을 둬서 각 token이 실제 token인지 generator가 만든 가짜 token인지 판별하는 학습을 시킴
- 이러한 Replaced token detection task는 input token 전체에 대해서 학습하기 때문에, 전체 token 중에 작은 부분만을 masking해서 학습하는 기존 MLM 보다 훨씬 효율적인 학습을 할 수 있으며, 이를 철저한 실험을 통해서 증명함
- 결과적으로 동일한 모델 크기, 데이터, 컴퓨팅 파워에 대해서 BERT를 outperform했음
- 이런 결과는 모델 크기가 작을 때 더욱 부각됨. 이 기법으로 single GPU 4일 학습한 모델이 GPT 보다 GLUE에서 좋은 성능을 얻음 (GPT의 학습량이 30배 많은데도 불구하고)
- 또한  RoBERTa와 XLNet의 1/4의 컴퓨팅으로 비슷한 성능을 뽑았고, 같은 양으로 학습을 시키면 outperform함

## 1. Introduction

- 현재 state-of-the-art representation learning method는 일종의 denoising autoencoder 학습이라고 볼 수 있음
- 주로 MLM이라는 원본 input에서 약 15% 정도의 token을 masking하고 이를 복원시키는 task를 통해 학습을 함
- 기존 (autoregressive) language modeling 학습에 비해 bidirectional 정보를 고려한다는 점에서 효과적인 학습을 할 수 있었음
- 하지만 이런 MLM 기반의 기법들은 문제가 있음
  1. 고작 15%만 밖에 학습을 못함
  2. (그래서) 학습하는데 비용이 많이 듬
  3. 학습 때는 $[MASK]$ token을 모델이 참고하여 예측하지만 실제(inference)로는 $[MASK]$ token이 들어오지 않음

- 이런 점들을 해결하기 위해 *replaced token detection* 이라는 새로운 pre-training task를 제안함
  - **Replaced Token detection**: 실제 input의 일부 token을 generator를 통해 그럴싸한 가짜 token으로 바꾸고 discriminator가 각 token이 원본에서 온 진짜인지 생성(sampling)해낸 가짜인지 맞추는 task
- 15%가 아닌 input 문장의 전체 token에 대해서 학습을 할 수 있어서 상당히 효율적이고 효과적임
- 얼핏 보면 GAN과 유사하지만 generator가 maximum likelihood로 학습한다는 점에서 adversarial은 아님

- 위와 같은 방식으로 학습시킨 pre-trained LM인 ELECTRA (for "Efficiently Learning an Encoder that Classifies Token Replacements Accurately")를 제안함
- ELECTRA는 앞선 설명과 같이 모든 token에 대해서 학습을 할 수 있어서 BERT보다 훨씬 빠르게 학습이 가능하고 최종 학습을 완료하면 downstream tasks에서 더 좋은 성능을 보임

![figure1](/assets/images/blog/2020-04-13-electra/figure1.png)

- Figure 1을 보면 ELECTRA가 다른 approaches에 비해서 매우 빠르게 성능이 향상되는 것을 볼 수 있음
- 같은 크기의 모델들에 비교하면 최종 성능 포함 모든 과정에서 더 높은 GLUE 성능을 보임
- ELECTRA-Small은 single GPU로 4일이면 학습이 가능 (이는 BERT-Large의 1/20 parameter, 1/135 computing에 해당하는 수치)
  - 그럼에도 불구하고 BERT-Small 보다 GLUE에서 5점이나 높고, 심지어는 GPT 보다도 높음
- Large scale에 대해서 역시 좋은 성능을 보임
  - ELECTRA-Large는 RoBERTa나 XLNet 보다 더 적은 parameter, 1/4의 computing으로 학습했지만 이들과 비슷한 성능을 보임
- GLUE에서 ALBERT (Lan et al., 2019) 보다도 outperform했고 SQuAD 2.0은 SOTA를 찍음
- 종합적으로 봤을 때, 제안한 discriminative task를 통해서 모델이 더 어려운 negative sample에 대해서 학습했고, 기존 language representation learning approaches보다 더 효과적이고 효율적인 학습을 함

## 2. Method

![figure1](/assets/images/blog/2020-04-13-electra/figure2.png)

-
