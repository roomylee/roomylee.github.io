---
title: "Towards a Human-like Open-Domain Chatbot"
layout: post
date: 2020-02-05 11:30
headerImage: false
tag:
- Meena
- open-domain-chatbot
- generation
- encoder-decoder
category: blog
author: roomylee
---

---

- Paper Link: [https://arxiv.org/abs/2001.09977](https://arxiv.org/abs/2001.09977)
- Official Implementation: [https://github.com/google-research/google-research/tree/master/meena/](https://github.com/google-research/google-research/tree/master/meena/)
- Author
  - Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, Quoc V. Le
  - Google Research and Google Brain
- Published at
  - arXiv 2020

- Joohong's Review
  - 리뷰

---

## Abstract

- Meena라는 multi-turn open-domain chatbot을 제안함
- Public domain social media conversations(maybe Raddit)으로부터 data mining과 filtering을 통해 만든 데이터로 end-to-end training을 함
- 2.6B parameter를 갖는 neural network(GPT-2가 1.5B)에 대해서 간단한 language modeling 학습을 함 (minimize perplexity of the next token)
- 또한 성능 평가를 위해 Sensibleness and Specificity Average(SSA)라는 human evaluation metric을 새롭게 제안함
- 우리는 실험을 통해 perplexity와 SSA 사이에 강한 상관관계가 있다는 것을 보임
- 가장 좋은 perplexity를 보였던 Meena 모델이 72%의 SSA를 기록함
- 사람이 86% 정도의 SSA 성능을 보이는 걸로 보아, perplexity를 더 minimize한다면 사람 수준에 도달할 수 있다고 봄
- 게다가, filtering mechanism과 decoding tunning을 통한 최종 full 버전의 Meena는 79%의 SSA를 기록하였고, 이는 기존에 존재하던 chatbot보다 절대 수치로 23%나 높은 결과임

## 1. Introduction

## 2. Evaluating chatbots

### 2.1. Measuring Human Likeness

### 2.2. Static Evaluation

### 2.3. Interactive Evaluation

### 2.4. Estimate of Human Performance

### 2.5. Evaluation of Cleverbot and DialoGPT

### 2.6. Evaluation of Mitsuku and XiaoIce

### 2.7. Automatic Evaluation

## 3. Meena chatbot

- 최근 end-to-end dialog model은 결국 둘 중 하나임
  1. Complext models with human-designed components
  2. Large neural network model (known as end-to-end models)
- End-to-end model이 가능성은 보여줬지만, 한계가 너무나도 명확함
- 과연 더 많은 데이터, 더 많은 parameter를 통해 end-to-end approach가 궁극적으로 우리가 도달하고자하는 high-guality 대화를 할 수 있을까? 혹은 이를 위해선 결국 여러 component로 구성된 시스템으로 가야 하는 걸까? 라는 고민이 항상 있음
- 이번 섹션에서는 large end-to-end model인 Meena에 대해서 알아볼 것임
- 우리는 위 질문에 대한 답은 결국 large end-to-end model이 거의 humanlike한 chat response를 생성할 수 있느냐에 달려있다고 봄

### 3.1. Training Data

- **학습을 위한 데이터셋은 public domain social conversations을 mining하고 filtering해서 만들었음 (아마 Raddit으로 추정)**
- Source 데이터
  - 다자간의 Message tree 형태로 구성되어있음.
  - 가장 첫 메세지가 root이고 이에 대한 replies들이 child node가 되는 구조임
  - Tree 상의 path에 있는 메세지 시퀀스를 대화라고 보고 각 메세지가 하나의 발화 턴이라고 봄
  - 각 발화를 response, 해당 발화 이전의 턴들(최대 7턴)을 context라고 정의함
  - **최종 training example은 (*context*, *response*)의 pair 형태로 만듬**
- Filtering
  - Generation 성능 향상을 위해 데이터를 filtering함
  - Filtering Condition (걸리면 해당 메세지 삭제)
    1. Subword의 수가 2개 이하 or 128개 이상이면 삭제
    2. Alphabetic character가 70% 이하면 삭제
    3. URL 포함하면 삭제
    4. Username에 "bot"이 들어가면 삭제
    5. 메세지가 100번 이상 반복되면 삭제
    6. Parent 메세지와 n-gram 기준으로 많이 겹치면 삭제
    7. 불쾌하거나 폭력적인 내용을 담고 있으면 삭제
  - 또한 인용을 위해 parent 메세지를 copy한 경우, 해당 부분을 메세지에서 삭제
  - 위의 filtering 로직에 의해 메세지가 제거되면 해당 메세지 기준으로 하부의 subtree를 다 날림
- **Filtering 후에 (*context*, *response*) pair는 총 8억(867M)개가 남음**
- **[Sentencepiece library](https://github.com/google/sentencepiece)를 이용해서 BPE 기반 tokenizing을 하였고, vocab의 크기는 8K임**
  - 8K는 초기 실험을 통해 generation하기에도 충분하면서 large model에 맞는 memory를 고려하여 잡았음
- **최종적으로 40B word로 구성된 341GB text 사용함. 참고로 GPT-2(Radford et al., 2019)는 40GB의 internet text(from 8M web pages)를 사용함**

### 3.2. Model Architecture

- **Evolved Transformer(ET) (So et al., 2019)** 기반의 **seq2seq(encoder-decoder) 모델**을 사용함
- **총 2.6B의 parameter를 가지며, 1개의 ET encoder block과 13개의 ET decoder block으로 구성됨**
- ET는 Transformer 기반의 evolutionary NAS architecture(Real et al., 2017, 2018)임
  - ET는 10.2의 perplexity, vanilla Transformer (32 decoder layers)는 10.7의 perplexity를 동일한 training step(738k)에서 기록하였음
  - 참고로 ET block은 Transformer layer에 비해 약 2배 정도 깊음
- 다른 모델과의 스펙 비교
  - 일단 parameter가 1.5B인 GPT-2보다 Meena가 2.6B로 더 많음
  - 또한 GPT-2는 language model로서 decoder만 있는 셈인데 비해, Meena는 encoder-decoder 구조임
  - 최근에 나온 large conversational model인 DialoGPT (Zhang et al., 2019)는 762M parameter를 갖고 있고 역시 Meena가 더 큼. 참고로 BERT는 110M 정도임
- Hyperparameters
  - hidden size = 2560
  - \# of attention head = 32
  - encoder, decoder, softmax layer의 embedding은 모두 share함
  - max sentence length = 128 (tokens)
  - 최적의 hyperparameter는 manual coordinate-descent search를 통해 찾았음

### 3.3. Training Details

- **TPU v3 Pod(2048 TPU cores)으로 30일동안 학습** (참고로 TPU v3-8 instance(8 TPU cores)가 $8.00/hour임. 대충 20억원 정도...)
  - 40B words (61B BPE tokens)로 구성된 데이터셋을 사용
  - **2.6B parameter의 모델이 이정도 데이터를 쓰니까 overfitting 현상이 보인다고 함!!**
    - Validation loss는 증가하고 train loss는 감소했다고 함
    - BERT(110M)는 Wikipedia(2.5B words) + BookCorpus(800M words)를 사용했으며, underfit하다는 의견이 많았음
  - **이런 차원에서 overfitting을 완화하기위해 feed-forward layer에 0.1 정도의 dropout을 추가하였음**
- 메모리를 절약하기 위해 Adafactor를 사용하였으며, 0.01의 initial learning rate을 설정하였음
  - 첫 10k step까지는 해당 lr 값을 유지하고, 이후부터 inverse square root of number of steps로 decaying 하였음
- Meena를 학습시키기 위해 [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) 사용하였음
- TPU 학습의 디테일
  - TPU-v3가 16GB의 메모리를 갖고 있고, 메모리 usage를 최대 쓸 수 있게 모델 크기(parameter)를 설정하였음
  - 그리고 core 당 8개의 training example만을 저장하였으며, 1 training step에 1초 정도가 걸렸음
  - TPU-v3 Pod으로 치면, training second 당 4M 개의 토큰을 학습하는 셈이고, 학습이 완료되었을 때 모델은 full training set을 164번(epochs) 보게 되며 중복 포함해서 총 10T개의 token을 보는 것임

### 3.4. Decoding

- Generation에서의 큰 challege 중 대표적으로 generic response 문제와 bland response 문제가 있음
- 이런 문제를 완화시키기 위해 아래와 같은 방법들 사용
  - Reranking (Li et al., 2016a; Shao et al., 2017)
  - Conditioning on profiles, topics, ans styles (Li et al., 2016b; Wang et al., 2017; Xing et al., 2017; Zhang et al., 2018b)
  - Adversarial learning (Li et al., 2017; Zhang et al., 2018c)
  - Variational autoencoder (Zhao et al., 2017; Gu et al., 2019)
  - Adversarial Learning + VAE (Gao et al., 2019b)
- **반면 우리는 충분히 low perplexity를 갖는 모델에 대해 간단한 "sample-and-rank decoding strategy"를 적용하였고 diverse하면서도 high-quality response를 얻을 수 있었음**

#### Sample-and-rank
- 방법
  1. N개의 independent candidate response를 temperature $T$ 를 이용해서 random sampling 한다
     - (정확히는 하나의 response를 만들 때 temperature 기반 random sampling을 사용하고, 이 방법으로 N개의 response를 독립적으로 생성해내는 것)
  2. Candidate 중 가장 확률이 높은 response를 최종 output으로 내보낸다
- Temperature $T > 0$ 는 decoding 시에 next token에 대한 확률 분포 $p_i$ 를 regulate하는 것임 (Hinton et al., 2015). $T = 1$ 는 원래의 distribution을 쓰겠다는 것

$$
p_i = \frac{exp(z_i/T)}{\sum_j{exp(z_j/T)}}
$$

- **$T$ 를 크게 가져가면, 분포가 스무스해지고 sampling 결과가 다양해짐. 따라서 relevant entity names과 같은 contextually rare token이 많이 나오게 됨. 그만큼 incorrect token에 큰 probability가 실리는 문제가 생기기도 함**
- **$T$ 를 작게 가져가면, 분포가 뾰족해지고 sampling 결과가 단조로워짐. 따라서 articles(관사)이나 prepositions(전치사)와 같이 상대적으로 common한, 보다 안전하면서도 구체적이지 않은 단어가 나옴**

![table2](/assets/images/blog/2020-02-05-meena/table2.png)

![table3](/assets/images/blog/2020-02-05-meena/table3.png){: .center}

- 위의 Table 2와 Table 3는 "Why do you like the ocean?" 이라는 임의로 뽑은 input에 대한 sample-and-rank와 beam-search 결과임
- 보다시피 beam-search의 결과는 repetive하고 uninteresting함. 반면 sample-and-rank는 diverse하고 content-rich한 response가 나옴
- **이 방법의 포인트는 결국 model이 충분히 적절한 문장(low perplexity)을 만들 수 있다는데 있음. 따라서 높은 temperature로 sampling을 하더라도 문장이 말이 되면서 human-like content를 담을 수 있다는 것임**
  - (반대로 생각해보면, 문장조차 제대로 못만드는 모델은 이런 sampling을 적용하기 적절치 않다는 얘기인듯. 이러한 이유로 지금까지 sample-and-rank보다 안정적인 beam-search를 널리 사용해왔지 않았나 싶음)
- Section 4의 모든 결과는 sample-and-rank를 할 때, $N = 20$, $T = 0.88$ 으로 한 결과임

### 3.5. Sample conversations

![sample_conversation](/assets/images/blog/2020-02-05-meena/sample_conversation.png){: .full}

## 4. Results

### 4.1 SSA-perplexity correlation

### 4.2. Human-level Estimates

### 4.3. XiaoIce, Mitsuku, DialoGPT, Cleverbot

### 4.4. Sample Responses: Meena(base), Cleverbot, and DialoGPT

## 5. Further Advancing SSA

### 5.1. Advancing Decoding

### 5.2. Addressing Cross-turn Repetitions

### 5.3. Safety Layer

## 6. Related Work

## 7. Discussion
